{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b260f32",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pyspark==3.4.0 pandas numpy scikit-learn openjdk-11-jdk-headless -q\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd7252",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SmartTollSystem\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\"Spark Session created successfully!\")\n",
    "print(f\"Version: {spark.version}\")\n",
    "print(f\"Partitions: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589aa6ff",
   "metadata": {},
   "source": [
    "## Step 3: Generate Sample Toll Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_toll_data(num_records=5000):\n",
    "    \"\"\"\n",
    "    Generate realistic sample toll transaction data\n",
    "    \"\"\"\n",
    "    print(f\"[DataGen] Generating {num_records} toll transaction records...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Time range\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    end_date = datetime(2023, 12, 31)\n",
    "    \n",
    "    # Parameters\n",
    "    plazas = [1, 2, 3, 4]\n",
    "    lanes = [1, 2, 3, 4]\n",
    "    vehicle_types = ['bike', 'car', 'truck', 'bus', 'heavy_vehicle']\n",
    "    payment_modes = ['wallet', 'cash', 'upi']\n",
    "    \n",
    "    # Toll amounts by vehicle type\n",
    "    toll_amounts = {\n",
    "        'bike': [20, 40],\n",
    "        'car': [40, 80],\n",
    "        'truck': [120, 200],\n",
    "        'bus': [80, 150],\n",
    "        'heavy_vehicle': [180, 250]\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    current_date = start_date\n",
    "    \n",
    "    while current_date <= end_date and len(data) < num_records:\n",
    "        hour = current_date.hour\n",
    "        day_of_week = current_date.weekday()\n",
    "        \n",
    "        # Determine number of vehicles in this hour\n",
    "        base_vehicles = 60\n",
    "        \n",
    "        # Peak hours boost\n",
    "        if (7 <= hour < 10) or (17 <= hour < 20):\n",
    "            base_vehicles = 150\n",
    "        \n",
    "        # Weekend lighter\n",
    "        if day_of_week >= 5:\n",
    "            base_vehicles = int(base_vehicles * 0.8)\n",
    "        \n",
    "        # Night is light\n",
    "        if hour < 6 or hour > 22:\n",
    "            base_vehicles = int(base_vehicles * 0.3)\n",
    "        \n",
    "        num_vehicles = max(0, int(base_vehicles + np.random.normal(0, 20)))\n",
    "        \n",
    "        # Generate transactions\n",
    "        for _ in range(num_vehicles):\n",
    "            if len(data) >= num_records:\n",
    "                break\n",
    "            \n",
    "            vehicle_type = random.choice(vehicle_types)\n",
    "            toll_amount = random.uniform(*toll_amounts[vehicle_type])\n",
    "            status = 'completed' if random.random() > 0.02 else 'failed'\n",
    "            \n",
    "            record = {\n",
    "                'txn_id': len(data) + 1,\n",
    "                'vehicle_id': random.randint(1, 500),\n",
    "                'plaza_id': random.choice(plazas),\n",
    "                'lane_no': random.choice(lanes),\n",
    "                'timestamp': current_date + timedelta(minutes=random.randint(0, 59)),\n",
    "                'amount': round(toll_amount, 2),\n",
    "                'vehicle_type': vehicle_type,\n",
    "                'payment_mode': random.choice(payment_modes),\n",
    "                'status': status\n",
    "            }\n",
    "            data.append(record)\n",
    "        \n",
    "        current_date += timedelta(hours=1)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"[DataGen] Generated {len(df)} records\")\n",
    "    print(f\"[DataGen] Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "pandas_df = generate_toll_data(num_records=5000)\n",
    "print(f\"\\n[Data] First few records:\")\n",
    "print(pandas_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bdde3c",
   "metadata": {},
   "source": [
    "## Step 4: Convert to Spark DataFrame and Perform Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aedc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "print(\"[Spark] DataFrame created\")\n",
    "spark_df.printSchema()\n",
    "print(f\"[Spark] Total records: {spark_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3f2df",
   "metadata": {},
   "source": [
    "### Analytics 1: Hourly Vehicle Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly aggregation\n",
    "hourly = spark_df.withColumn(\n",
    "    \"timestamp\",\n",
    "    F.col(\"timestamp\").cast(TimestampType())\n",
    ").withColumn(\n",
    "    \"date\",\n",
    "    F.date_trunc(\"day\", F.col(\"timestamp\"))\n",
    ").withColumn(\n",
    "    \"hour\",\n",
    "    F.hour(F.col(\"timestamp\"))\n",
    ").groupBy(\n",
    "    F.col(\"plaza_id\"),\n",
    "    F.col(\"date\"),\n",
    "    F.col(\"hour\")\n",
    ").agg(\n",
    "    F.count(\"*\").alias(\"vehicle_count\"),\n",
    "    F.sum(\"amount\").alias(\"total_revenue\")\n",
    ").orderBy(\n",
    "    F.col(\"plaza_id\"),\n",
    "    F.col(\"date\"),\n",
    "    F.col(\"hour\")\n",
    ")\n",
    "\n",
    "print(\"[Analytics] Hourly Vehicle Aggregation (Sample):\")\n",
    "hourly.limit(15).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999da424",
   "metadata": {},
   "source": [
    "### Analytics 2: Revenue Per Plaza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0fcc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue per plaza\n",
    "revenue_per_plaza = spark_df.groupBy(F.col(\"plaza_id\")).agg(\n",
    "    F.count(\"*\").alias(\"total_vehicles\"),\n",
    "    F.sum(\"amount\").alias(\"total_revenue\"),\n",
    "    F.avg(\"amount\").alias(\"avg_toll_amount\")\n",
    ").orderBy(F.desc(\"total_revenue\"))\n",
    "\n",
    "print(\"[Analytics] Revenue Per Plaza:\")\n",
    "revenue_per_plaza.show()\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "revenue_pandas = revenue_per_plaza.toPandas()\n",
    "print(\"\\n[Pandas] Revenue Data:\")\n",
    "print(revenue_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4aad5",
   "metadata": {},
   "source": [
    "### Analytics 3: Vehicle Type Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle type aggregation\n",
    "vehicle_type_stats = spark_df.groupBy(F.col(\"vehicle_type\")).agg(\n",
    "    F.count(\"*\").alias(\"vehicle_count\"),\n",
    "    F.sum(\"amount\").alias(\"total_revenue\"),\n",
    "    F.avg(\"amount\").alias(\"avg_toll_amount\")\n",
    ").orderBy(F.desc(\"vehicle_count\"))\n",
    "\n",
    "print(\"[Analytics] Vehicle Type Distribution:\")\n",
    "vehicle_type_stats.show()\n",
    "\n",
    "# Pie chart\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vehicle_pandas = vehicle_type_stats.toPandas()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(vehicle_pandas['vehicle_count'], labels=vehicle_pandas['vehicle_type'], autopct='%1.1f%%')\n",
    "plt.title('Vehicle Count Distribution by Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa1233d",
   "metadata": {},
   "source": [
    "### Analytics 4: Peak Hours Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e531e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak hours\n",
    "peak_hours = spark_df.withColumn(\n",
    "    \"timestamp\",\n",
    "    F.col(\"timestamp\").cast(TimestampType())\n",
    ").withColumn(\n",
    "    \"hour\",\n",
    "    F.hour(F.col(\"timestamp\"))\n",
    ").groupBy(\n",
    "    F.col(\"hour\")\n",
    ").agg(\n",
    "    F.count(\"*\").alias(\"vehicle_count\"),\n",
    "    F.sum(\"amount\").alias(\"total_revenue\")\n",
    ").orderBy(F.desc(\"vehicle_count\")).limit(5)\n",
    "\n",
    "print(\"[Analytics] Top 5 Peak Hours:\")\n",
    "peak_hours.show()\n",
    "\n",
    "# Bar chart\n",
    "peak_pandas = peak_hours.toPandas()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(peak_pandas['hour'], peak_pandas['vehicle_count'], color='skyblue')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Vehicle Count')\n",
    "plt.title('Peak Traffic Hours')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26bac0",
   "metadata": {},
   "source": [
    "### Analytics 5: Traffic Log Creation (for ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create traffic log with traffic level classification\n",
    "traffic_log = spark_df.withColumn(\n",
    "    \"timestamp\",\n",
    "    F.col(\"timestamp\").cast(TimestampType())\n",
    ").withColumn(\n",
    "    \"date\",\n",
    "    F.date_trunc(\"day\", F.col(\"timestamp\"))\n",
    ").withColumn(\n",
    "    \"hour\",\n",
    "    F.hour(F.col(\"timestamp\"))\n",
    ").groupBy(\n",
    "    F.col(\"plaza_id\"),\n",
    "    F.col(\"date\"),\n",
    "    F.col(\"hour\")\n",
    ").agg(\n",
    "    F.count(\"*\").alias(\"vehicle_count\"),\n",
    "    F.sum(\"amount\").alias(\"total_revenue\")\n",
    ")\n",
    "\n",
    "# Classify traffic level\n",
    "p50 = traffic_log.approxQuantile(\"vehicle_count\", [0.5], 0.01)[0]\n",
    "p75 = traffic_log.approxQuantile(\"vehicle_count\", [0.75], 0.01)[0]\n",
    "\n",
    "traffic_log = traffic_log.withColumn(\n",
    "    \"traffic_level\",\n",
    "    F.when(F.col(\"vehicle_count\") > p75, \"high\") \\\n",
    "        .when(F.col(\"vehicle_count\") > p50, \"normal\") \\\n",
    "        .otherwise(\"low\")\n",
    ")\n",
    "\n",
    "print(f\"[Analytics] Traffic Log Statistics:\")\n",
    "print(f\"  50th percentile (median): {p50:.0f} vehicles\")\n",
    "print(f\"  75th percentile: {p75:.0f} vehicles\")\n",
    "print(f\"\\n[Analytics] Sample Traffic Log:\")\n",
    "traffic_log.limit(20).show()\n",
    "\n",
    "# Convert to Pandas for ML\n",
    "traffic_log_pandas = traffic_log.toPandas()\n",
    "print(f\"\\n[Data] Total traffic log records: {len(traffic_log_pandas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94801308",
   "metadata": {},
   "source": [
    "## Step 5: Machine Learning - Train Traffic Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "print(\"\\n[ML] Preparing data for model training...\")\n",
    "\n",
    "# Prepare data\n",
    "ml_data = spark_df.withColumn(\n",
    "    \"timestamp\",\n",
    "    F.col(\"timestamp\").cast(TimestampType())\n",
    ").withColumn(\n",
    "    \"hour\",\n",
    "    F.hour(F.col(\"timestamp\"))\n",
    ").withColumn(\n",
    "    \"day_of_week\",\n",
    "    F.dayofweek(F.col(\"timestamp\"))\n",
    ").select(\n",
    "    \"plaza_id\", \"hour\", \"day_of_week\", \"amount\"\n",
    ")\n",
    "\n",
    "# Convert to Pandas\n",
    "ml_pandas = ml_data.toPandas()\n",
    "\n",
    "# Add features\n",
    "ml_pandas['is_peak_hour'] = (\n",
    "    ((ml_pandas['hour'] >= 7) & (ml_pandas['hour'] < 10)) | \n",
    "    ((ml_pandas['hour'] >= 17) & (ml_pandas['hour'] < 20))\n",
    ").astype(int)\n",
    "\n",
    "# Group by hour to get vehicle count as target\n",
    "hourly_data = ml_pandas.groupby(['plaza_id', 'hour', 'day_of_week', 'is_peak_hour']).size().reset_index(name='vehicle_count')\n",
    "\n",
    "print(f\"[ML] Created {len(hourly_data)} hourly records for training\")\n",
    "\n",
    "# Features and target\n",
    "X = hourly_data[['plaza_id', 'hour', 'day_of_week', 'is_peak_hour']].values\n",
    "y = hourly_data['vehicle_count'].values\n",
    "\n",
    "print(f\"[ML] Feature shape: {X.shape}, Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19d1a2",
   "metadata": {},
   "source": [
    "### Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d22b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest model\n",
    "print(\"\\n[ML] Training Random Forest model...\")\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"\\n[ML] ===== TRAINING RESULTS =====\")\n",
    "print(f\"Train R² Score: {r2_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Test R² Score:  {r2_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"Train RMSE:     {np.sqrt(mean_squared_error(y_train, y_pred_train)):.2f}\")\n",
    "print(f\"Test RMSE:      {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")\n",
    "print(f\"Train MAE:      {mean_absolute_error(y_train, y_pred_train):.2f}\")\n",
    "print(f\"Test MAE:       {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_names = ['plaza_id', 'hour', 'day_of_week', 'is_peak_hour']\n",
    "print(\"\\n[ML] Feature Importance:\")\n",
    "for name, importance in zip(feature_names, model.feature_importances_):\n",
    "    print(f\"  {name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107f4c8",
   "metadata": {},
   "source": [
    "### Visualize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Feature importance\n",
    "axes[0, 0].barh(feature_names, model.feature_importances_, color='skyblue')\n",
    "axes[0, 0].set_xlabel('Importance')\n",
    "axes[0, 0].set_title('Feature Importance')\n",
    "axes[0, 0].grid(axis='x')\n",
    "\n",
    "# Actual vs Predicted (Test set)\n",
    "axes[0, 1].scatter(y_test, y_pred_test, alpha=0.6)\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Actual Vehicle Count')\n",
    "axes[0, 1].set_ylabel('Predicted Vehicle Count')\n",
    "axes[0, 1].set_title('Actual vs Predicted (Test Set)')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1, 0].scatter(y_pred_test, residuals, alpha=0.6)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Predicted Values')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].set_title('Residual Plot')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Error distribution\n",
    "axes[1, 1].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Residual Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Residual Distribution')\n",
    "axes[1, 1].grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"[ML] Model visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14be669d",
   "metadata": {},
   "source": [
    "## Step 6: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c37a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for future hours\n",
    "print(\"\\n[Prediction] Predicting traffic for next 6 hours...\\n\")\n",
    "\n",
    "current_hour = datetime.now().hour\n",
    "current_day = datetime.now().weekday()\n",
    "plaza_id = 1\n",
    "\n",
    "predictions = []\n",
    "for i in range(1, 7):\n",
    "    hour = (current_hour + i) % 24\n",
    "    is_peak = 1 if (7 <= hour < 10) or (17 <= hour < 20) else 0\n",
    "    \n",
    "    # Create feature array\n",
    "    features = np.array([[plaza_id, hour, current_day, is_peak]])\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Predict\n",
    "    predicted_vehicles = max(0, int(model.predict(features_scaled)[0]))\n",
    "    \n",
    "    # Determine traffic level\n",
    "    if predicted_vehicles > p75:\n",
    "        traffic_level = 'HIGH'\n",
    "    elif predicted_vehicles > p50:\n",
    "        traffic_level = 'NORMAL'\n",
    "    else:\n",
    "        traffic_level = 'LOW'\n",
    "    \n",
    "    predictions.append({\n",
    "        'Hour': f\"{hour:02d}:00\",\n",
    "        'Predicted Vehicles': predicted_vehicles,\n",
    "        'Traffic Level': traffic_level,\n",
    "        'Confidence': '75%'\n",
    "    })\n",
    "    \n",
    "    print(f\"Hour {hour:02d}:00 - Predicted: {predicted_vehicles} vehicles - Level: {traffic_level}\")\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "print(\"\\n[Prediction] Summary:\")\n",
    "print(pred_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11358da6",
   "metadata": {},
   "source": [
    "## Step 7: Summary & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SMART TOLL MANAGEMENT SYSTEM - ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[DATA GENERATION]\")\n",
    "print(f\"  Total Records: {len(pandas_df):,}\")\n",
    "print(f\"  Time Period: {pandas_df['timestamp'].min()} to {pandas_df['timestamp'].max()}\")\n",
    "print(f\"  Toll Plazas: 4\")\n",
    "print(f\"  Total Revenue: Rs. {pandas_df['amount'].sum():.2f}\")\n",
    "print(f\"  Average Toll: Rs. {pandas_df['amount'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n[SPARK ANALYTICS]\")\n",
    "print(f\"  Hourly Records Generated: {len(traffic_log_pandas):,}\")\n",
    "print(f\"  Peak Hour Vehicle Count: {traffic_log_pandas['vehicle_count'].max()}\")\n",
    "print(f\"  Low Traffic Vehicle Count: {traffic_log_pandas['vehicle_count'].min()}\")\n",
    "\n",
    "print(\"\\n[MACHINE LEARNING]\")\n",
    "print(f\"  Model Type: Random Forest Regressor\")\n",
    "print(f\"  Training Samples: {len(X_train):,}\")\n",
    "print(f\"  Test Samples: {len(X_test):,}\")\n",
    "print(f\"  Test R² Score: {r2_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}\")\n",
    "print(f\"  Test MAE: {mean_absolute_error(y_test, y_pred_test):.2f}\")\n",
    "\n",
    "print(\"\\n[KEY INSIGHTS]\")\n",
    "print(f\"  Most Important Feature: {feature_names[np.argmax(model.feature_importances_)]}\")\n",
    "print(f\"  Peak Hour Traffic: {p75:.0f}+ vehicles\")\n",
    "print(f\"  Normal Hour Traffic: {p50:.0f}-{p75:.0f} vehicles\")\n",
    "print(f\"  Low Traffic: <{p50:.0f} vehicles\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Analysis Complete! Ready for deployment.\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
